---
title: "Fatal Police Shootings"
author: "Devin Luu, Steven Villarreal, Yifan Lyu, Matthew Strasburg"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.world)
library(shiny)
library(gridExtra)
library(MASS)
library(ROCR)
library(pROC)
knitr::opts_chunk$set(echo = TRUE)
```
  
## **R Session Info**  

```{r}
sessionInfo()
```

## **Github Link** 
https://github.com/devtl/policeShootings

## **Introduction** 
Fall 2017 classification project. Objective is to compare the quality of classification algorithms used (logistic regression, linear discriminant analysis, quadratic discriminant analysis, and kth nearest neighbors) to predict perceived threat levels based on several factors.

Dataset is stored on https://data.world/devtl/fatalpoliceshootings.

## **Getting data from data.world** 
Short description of the data.
```{r}
project <- "https://data.world/devtl/fatalpoliceshootings/"
data.world::set_config(cfg_env("DW_API"))
df <- data.world::query(data.world::qry_sql("SELECT * FROM fatalPoliceShootingsCleaned"), dataset = project)

summary(df)
```

## Possible classifications

This dataset has many qualitative variables that could do well for classification methods. I am interested in predicting a victim's threat level based on age, whether or not they are armed, age, gender, race, whether or not they are fleeing, and signs of mental illness.

```{r}
predictors <- df%>%dplyr::select("armed","age","gender","race","signs_of_mental_illness","flee")

# signs of mental illness: 1=T, 0=F
x <- factor(rep(predictors$signs_of_mental_illness))
levels(x) <- 0:1
predictors$signs_of_mental_illness = x

# fleeing: 3=car, 2=foot, 1=other, 0=not fleeing
y <- factor(rep(predictors$flee))
levels(y) = c(3,2,0,1)
predictors$flee <- y

# gender: 1=M, 0=F
w <- factor(rep(predictors$gender))
levels(w) <- 0:1
predictors$gender = w

# armed=1 or unarmed=0
z <- unlist(
lapply(
predictors$armed, function(s){ifelse(s == "unarmed", as.integer(0), as.integer(1))}
)
)
predictors <- predictors %>% mutate(armedOrUnarmed = z)
renderTable(head(predictors), hover = TRUE, width = "auto")
```

Value Mappings:

* signs of mental illness: 1=T, 0=F

* fleeing: 3=car, 2=foot, 1=other, 0=not fleeing

* gender: 1=M, 0=F

* armed=1 or unarmed=0

## Traing/Testing

Our data will be split 90/10 for training and testing purposes, respectively. The split will be randomly sampled using R's sample() function. 

```{r}
### TRAIN/TEST 

df1 <- cbind(threat = df$threat_level,predictors)
train <- sample(1:nrow(df1), 0.9*nrow(df1))
testDf <- na.omit(df1[-train,])
trainDf <- na.omit(df1[train,])
```

## Classification Methods

### Logistic Regression
```{r}
# after filtering out a class level since lr is a binomial method.
df2 <- filter(df1, threat != "undetermined")
testDf2 <- filter(testDf, threat != "undetermined")
testDf2$threat <- factor(testDf2$threat) # removes "undetermined" level from testDf2

train2 <- rownames(
  filter(trainDf, 
         threat != "undetermined")
  )

### LOG REGRESSION
logFit1 = glm(threat ~ age + gender + flee + signs_of_mental_illness + armedOrUnarmed, 
            data = df2, 
            family = binomial, 
            subset = train2)

summary(logFit1)
logProb1 <- predict(logFit1, newdata = testDf2, type="response")

# label classes based on probability threshold for 'other'
logPred1 <- rep("attack", length(logProb1))

logPred1[logProb1 > 0.5] <- "other"
testGroup1 = testDf2$threat
table(logPred1, testGroup1)

logTestErr <- 1- mean(logPred1 == testGroup1)
```
The test error for the given LDA model is `r format(logTestErr, digits = 2)`.


### LDA

```{r}
### LDA
lda.fit <- lda(threat ~ age + gender + flee + signs_of_mental_illness + armedOrUnarmed, data = df1, subset = train)

lda.fit

ldaPred <- predict(lda.fit,testDf)

#renderTable({head(ldaPred)})

renderPlot({
  ggplot(data.frame(ldaPred), mapping = aes(x.LD1, x.LD2, color = class)) + geom_point(alpha = 0.7) + theme_minimal() + labs(x = "LD1", y = "LD2")
})


testGroup = testDf$threat
ldaClass=ldaPred$class

pivotTab <- table(ldaClass, testGroup)

ldaTestErr <- 1 - mean(ldaClass == testGroup)

pivotTab

```

The test error for the given LDA model is `r format(ldaTestErr, digits = 2)`.

### Quadratic Discriminant Analysis
```{r}
qda.fit <- qda(threat ~ age + gender + flee + signs_of_mental_illness + armedOrUnarmed, data = df1, subset = train)

qda.fit

qdaPred <- predict(qda.fit,testDf)

qdaClass <- qdaPred$class
testGroup = testDf$threat
table(qdaClass, testGroup)

qdaTestErr <- 1 - mean(qdaClass == testGroup)
```
The test error for the given LDA model is `r format(qdaTestErr, digits = 2)`.

### K Nearest-Neighbors

With K = 5:

```{r}
### TEST/TRAIN
testDf3 <- dplyr::select(na.omit(df1[-train,]), -armed, -threat, -race)
trainDf3<- dplyr::select(na.omit(df1[train,]), -armed, -threat, -race)
trainThreat3 <- dplyr::select(na.omit(df1[train,]), threat)

### KNN
library(class)

knnPred <- knn(trainDf3, testDf3, trainThreat3$threat, k=5)

testThreat3 <- dplyr::select(na.omit(df1[-train,]), threat)
table(knnPred,testThreat3$threat)
knnTestErr <- 1 - mean(knnPred == testThreat3$threat)
```
The test error for the given KNN model is `r format(knnTestErr, digits = 2)`.

### Error Comparison

```{r}
renderTable({
  tibble(Method = c("Logit", "LDA", "QDA", "5NN"), TestError = c(format(logTestErr, digits = 2), format(ldaTestErr, digits = 2), format(qdaTestErr, digits = 2), format(knnTestErr, digits = 2)))
  
})
```

## ROC Curves

### Logistic Regression

```{r}
glmPred <- predict(logFit1, newdata = testDf2)
glmPred <- lapply(glmPred, function(x){ifelse(x>0, 2, 1)}) # 1=attack, 2=other

ROCPred <- as.vector(glmPred, mode = "numeric")

ROC <- roc(testGroup1, as.numeric(ROCPred))

renderPlot({
  
  plot(ROC, main="ROC curve Theat Levels", xlab="Specificity (False Positives)", 
     ylab="Sensitivity (True Positives)")  
})

ROC
```

### LDA

```{r}

ROCPred <- ldaPred
levels(ROCPred$class) <- c(0,1,2) #attack = 0, other = 1, undetermined = 2
multiROC <- multiclass.roc(testGroup,as.numeric(ROCPred$class))

rs <- multiROC$rocs
renderPlot({
  par(mfrow = c(1,3))
  plot(rs[[1]], print.auc = TRUE, main="ROC curve Theat Levels [[1]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  plot(rs[[2]], print.auc = TRUE, main="ROC curve Theat Levels [[2]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  plot(rs[[3]], print.auc = TRUE, main="ROC curve Theat Levels [[3]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  
})

rs

```

### QDA

```{r}

ROCPred2 <- qdaPred
levels(ROCPred2$class) <- c(0,1,2)

multiROC2 <- multiclass.roc(testGroup, as.numeric(ROCPred2$class))

rs2 <- multiROC2$roc

renderPlot({
  par(mfrow=c(1,3))
  plot(rs2[[1]], print.auc = TRUE, main="ROC curve Theat Levels [[1]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  plot(rs2[[2]], print.auc = TRUE, main="ROC curve Theat Levels [[2]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  plot(rs2[[3]], print.auc = TRUE, main="ROC curve Theat Levels [[3]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  
})

rs2

```

### KNN

```{r}

ROCPred3 <- knnPred

multiROC3 <- multiclass.roc(testThreat3$threat, as.numeric(ROCPred3))

rs3 <- multiROC3$rocs

renderPlot({
  par(mfrow=c(1,3))
  plot(rs3[[1]], print.auc = TRUE, main="ROC curve Theat Levels [[1]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  plot(rs3[[2]], print.auc = TRUE, main="ROC curve Theat Levels [[2]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  plot(rs3[[3]], print.auc = TRUE, main="ROC curve Theat Levels [[3]]", xlab="Specificity (False Positives)", ylab="Sensitivity (True Positives)")
  
})

rs3
```

## Conclusion

Our predictors, in general, were not significant enough to determine percieved threat levels among police victim shootings as event by the poor AUC levels in our ROC curves. Of the 4 models used (logistic regression, linear discriminant analysis, quadratic discriminant analysis, and 5-nearest neighbors), it seems that LDA and QDA produce similar overall results for a 3-class category ("attack", "other", and "undetermined"). Logistic regression may have a lower test error because it only had to predict between "attack" and "other"("undetermined" was removed because it was the smallest class and had the least effect on the overall model).